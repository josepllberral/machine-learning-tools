% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/wrapper.R
\name{train.cnn}
\alias{train.cnn}
\title{Training a Convolutional Neural Network or MultiLayer Perceptron Function}
\usage{
train.cnn(dataset, targets, layers = NULL, evaluator = NULL,
  batch_size = 10, training_epochs = 10, learning_rate = 0.001,
  momentum = 0.8, rand_seed = 1234, init_cnn = NULL)
}
\arguments{
\item{dataset}{A matrix with data, one example per row.}

\item{targets}{A matrix with output labels, one set of targets per row.}

\item{layers}{A list of layer descriptors (character vector).}

\item{evaluator}{An evaluator descriptor (character vector). Default "XENT".}

\item{batch_size}{Number of examples per training mini-batch. Default = 1.}

\item{training_epochs}{Number of training epochs. Default = 1000.}

\item{learning_rate}{The learning rate for training. Default = 0.01.}

\item{momentum}{The momentum for training. Default = 0.8. (Not Implemented Yet!)}

\item{rand_seed}{Random seed. Default = 1234.}
}
\description{
This function trains a CNN or MLP. Admits as parameters the training dataset,
the matrix of outputs, and a descriptor of the network including all the
layers and their properties. Returns a CNN in list form, including all the
trained layers, if input is a 4D dataset (samples x image with depth), or an
MLP in list form, including all the trained layers, if input is a 2D dataset
(samples x features).
Possible layers are:
\itemize{
  \item CONV: Convolutional Layer. Requires, in the following order:
  \enumerate{
    \item Number of channels of the input image
    \item Number of filters to be returned
    \item Size of convolutional filters
    \item Scale for initialization weights
    \item Border Mode (1 = valid, 2 = same, 3 = full)
  }
  \item POOL: Pooling Layer. Requires, in the following order:
  \enumerate{
    \item Number of channels of the input image.
    \item Scale for initialization weights
    \item Window size
    \item Stride
  }
  \item RELU: Rectified Linear. Requires, in the following order:
  \enumerate{
    \item Number of channels of the input image.
  }
  \item FLAT: Flattening Layer. Requires, in the following order:
  \enumerate{
    \item Number of channels of the input image.
  }
  \item LINE: Linear Layer. Requires, in the following order:
  \enumerate{
    \item Number of visible units
    \item Number of hidden units
    \item Scale for initialization weights
  }
  \item RBML: GB-RBM Layer. Requires, in the following order:
  \enumerate{
    \item Number of visible units
    \item Number of hidden units
    \item Scale for initialization weights
    \item Number of Gibbs Samplings at Backwards
  }
  \item RELV: Rectified Linear (for flattened batches). Does not require
  parameters
  \item SOFT: SoftMax Layer. Requires, in the following order:
  \enumerate{
    \item Number of visible units
  }
  \item SIGM: Sigmoid Layer. Requires, in the following order:
  \enumerate{
    \item Number of visible units
  }
  \item TANH: Hyperbolic Tangent Layer. Requires, in the following order:
  \enumerate{
    \item Number of visible units
  }
  \item DIRE: Direct (buffer) Layer. Requires, in the following order:
  \enumerate{
    \item Number of visible units
  }
}
The list of layers and dimensions is checked before the process starts.
Convolutional, Pooling, Rectifiers and Flattening layers can only be used
in CNNs. Other layers can be applied to CNNs and MLPs.
Also, evaluators can be changed. By default Cross-Entropy is used, but RBMs
can be specified to create Deep-Belief Networks.
Possible evaluators are:
\itemize{
  \item XENT: Cross-Entropy Loss layer. Requires no parameters.
  \enumerate{
    \item This is the Default, if no evaluator parameter is introduced.
  }
  \item RBML: GB-RBM Layer. Requires, in the following order:
  \enumerate{
    \item Number of visible units
    \item Number of hidden units
    \item Scale for initialization weights
    \item Number of Gibbs Samplings at Backwards
  }
}
The evaluator is checked before the process starts.
}
\examples{
## Simple example with CNN
train_X <- array(c(1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
                   0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0),
                   c(6, 1, 2, 3));
train_Y <- array(c(1, 0,
                   1, 0,
                   1, 1,
                   0, 0,
                   0, 1,
                   0, 1), c(6, 2));

layers <- list(
c('type' = "CONV", 'n_channels' = 1, 'n_filters' = 4, 'filter_size' = 5, 'scale' = 0.1, 'border_mode' = 'same'),
c('type' = "POOL", 'n_channels' = 4, 'scale' = 0.1, 'win_size' = 3, 'stride' = 2),
c('type' = "RELU", 'n_channels' = 4),
c('type' = "FLAT", 'n_channels' = 4),
c('type' = "LINE", 'n_visible' = 8, 'n_hidden' = 2, 'scale' = 0.1),
c('type' = "SOFT", 'n_inputs' = 2)
);

cnn1 <- train.cnn(train_X, train_Y, layers, batch_size = 2);

test_X <- array(c(1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
                  0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0),
                  c(6, 1, 2, 3));
results <- predict(cnn1, test_X);

## The MNIST example with CNN
data(mnist)

img_size <- c(28,28);

train <- mnist$train;
training_x <- array(train$x, c(nrow(train$x), 1, img_size)) / 255;
training_y <- binarization(train$y);

test <- mnist$test;
testing_x <- array(test$x, c(nrow(test$x), 1, img_size)) / 255;
testing_y <- binarization(test$y);

dataset <- training_x[1:1000,,,, drop=FALSE];
targets <- training_y[1:1000,, drop=FALSE];

newdata <- testing_x[1:1000,,,, drop=FALSE];

layers <- list(
c('type' = "CONV", 'n_channels' = 1, 'n_filters' = 4, 'filter_size' = 5, 'scale' = 0.1, 'border_mode' = 'same'),
c('type' = "POOL", 'n_channels' = 4, 'scale' = 0.1, 'win_size' = 3, 'stride' = 2),
c('type' = "RELU", 'n_channels' = 4),
c('type' = "CONV", 'n_channels' = 4, 'n_filters' = 16, 'filter_size' = 5, 'scale' = 0.1, 'border_mode' = 'same'),
c('type' = "POOL", 'n_channels' = 16, 'scale' = 0.1, 'win_size' = 3, 'stride' = 2),
c('type' = "RELU", 'n_channels' = 16),
c('type' = "FLAT", 'n_channels' = 16),
c('type' = "LINE", 'n_visible' = 784, 'n_hidden' = 64, 'scale' = 0.1),
c('type' = "RELV"),
c('type' = "LINE", 'n_visible' = 64, 'n_hidden' = 10, 'scale' = 0.1),
c('type' = "SOFT", 'n_inputs' = 10)
);

mnist_cnn <- train.cnn(dataset, targets, layers, batch_size = 10, training_epochs = 3,
                       learning_rate = 1e-3, momentum = 0.8, rand_seed = 1234);

prediction <- predict(mnist_cnn, newdata);

# Now introducing Evaluators (available for both CNNs and MLPs)
# - By default, Cross-Entropy is used (XENT)

eval <- c('type' = "XENT");
mnist_cnn <- train.cnn(dataset, targets, layers, evaluator = eval, batch_size = 10,
                       training_epochs = 3, learning_rate = 1e-3, rand_seed = 1234);

# - Also RBMs can be used for Deep-Belief Networks

eval <- c('type' = "RBML", 'n_visible' = 10, 'n_hidden' = 5, 'scale' = 0.1, 'n_gibbs' = 4);
mnist_dbn <- train.cnn(dataset, targets, layers, evaluator = eval, batch_size = 10,
                       training_epochs = 3, learning_rate = 1e-3, rand_seed = 1234);

## Simple example with MLPs
train_X <- array(c(1, 1, 1, 0, 0, 0,
                   1, 0, 1, 0, 0, 0,
                   1, 1, 1, 0, 0, 0,
                   0, 0, 1, 1, 1, 0,
                   0, 0, 1, 0, 1, 0,
                   0, 0, 1, 1, 1, 0),
                   c(6, 6));
train_Y <- array(c(1, 0,
                   1, 0,
                   1, 1,
                   0, 0,
                   0, 1,
                   0, 1), c(6, 2));

layers <- list(
   c('type' = "LINE", 'n_visible' = 6, 'n_hidden' = 2, 'scale' = 0.1),
   c('type' = "RELV"),
   c('type' = "SOFT", 'n_inputs' = 2)
);
mlp1 <- train.cnn(train_X, train_Y, layers, batch_size = 2);

test_X <- array(c(1, 1, 1, 0, 0, 0,
                  1, 0, 1, 0, 0, 0,
                  1, 1, 1, 0, 0, 0,
                  0, 0, 1, 1, 1, 0,
                  0, 0, 1, 0, 1, 0,
                  0, 0, 1, 1, 1, 0),
                  c(6, 6));

results <- predict(mlp1, test_X);

## The MNIST example with MLPs
data(mnist)

train <- mnist$train;
training_x <- train$x / 255;
training_y <- binarization(train$y);

test <- mnist$test;
testing_x <- test$x / 255;
testing_y <- binarization(test$y);

dataset <- training_x[1:1000,, drop=FALSE];
targets <- training_y[1:1000,, drop=FALSE];

newdata <- testing_x[1:1000,, drop=FALSE];

layers <- list(
   c('type' = "LINE", 'n_visible' = 784, 'n_hidden' = 64, 'scale' = 0.1),
   c('type' = "RELV"),
   c('type' = "LINE", 'n_visible' = 64, 'n_hidden' = 10, 'scale' = 0.1),
   c('type' = "SOFT", 'n_inputs' = 10)
);

mnist_mlp <- train.cnn(dataset, targets, layers, batch_size = 10, training_epochs = 10,
                       learning_rate = 1e-3, momentum = 0.8, rand_seed = 1234);

prediction <- predict(mnist_mlp, newdata);
}
\keyword{CNN}
\keyword{MLP}

