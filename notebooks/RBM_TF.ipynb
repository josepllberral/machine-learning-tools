{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBMs in Tensorflow\n",
    "\n",
    "## RBM training and testing algorithms, in TensorFlow\n",
    "\n",
    "@authors Josep Ll. Berral (Barcelona Supercomputing Center)\n",
    "\n",
    "@date 27th June, 2018\n",
    "\n",
    "@license GNU GPL v3 (or posterior) <http://www.gnu.org/licenses/>\n",
    "\n",
    "Here you can find a Restricted Boltzmann Machines implementation in TF, for academic and educational purposes.\n",
    "\n",
    "### References\n",
    "\n",
    "* Approach based on XXX RBM: TODO\n",
    "\n",
    "### Mocap data\n",
    "\n",
    "* Original file: TODO\n",
    "* Data originally from TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliar functions\n",
    "\n",
    "* Xavier Glorot's initialization method\n",
    "* Sampling Bernoulli\n",
    "* Sampling Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def glorot_init(fan_in, fan_out, const = 1.0, dtype = np.float32):\n",
    "    k = const * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out), minval = -k, maxval = k, dtype = dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_bernoulli(probs):\n",
    "    return tf.nn.relu(tf.sign(probs - tf.random_uniform(tf.shape(probs))))\n",
    "\n",
    "def sample_gaussian(x, sigma):\n",
    "    return x + tf.random_normal(tf.shape(x), mean = 0.0, stddev = sigma, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBM definition\n",
    "\n",
    "### Generic multi-purpose RBM\n",
    "\n",
    "Constructor:\n",
    "* creates the RBM structure and TF execution tree. Defines the number of visible units (inputs) and number of hidden units, also the hyperparameters.\n",
    "\n",
    "Training:\n",
    "* **fit**: trains the RBM using the provided inputs\n",
    "* **evaluate_error**: evaluates the reconstruction error of passing data forward and backward through the RBM\n",
    "\n",
    "Predicting/Reconstructing:\n",
    "* **forward**: propagates the inputs through the RBM towards the hidden layer activations\n",
    "* **backward**: propagates the activations through the RBM backwards to the visible layer input reconstructions\n",
    "* **forward_backward**: propagates the inputs through the RBM towards the hidden layer activations, then the activations backwards to the visible layer input reconstructions\n",
    "* **gibbs_sampling**: reconstructs inputs using Gibbs sampling passing inputs forward and backward the RBM\n",
    "\n",
    "Setters and Getters:\n",
    "* **get_weights**: gets the weights of the RBM for an external source\n",
    "* **set_weights**: sets the weights of the RBM from an external source\n",
    "* **save_weights**: saves the weights of the RBM into a file\n",
    "* **load_weights**: loads the weights of the RBM from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Gaussian-Bernoulli RBM\n",
    "class RBM:\n",
    "    \n",
    "    # Constructor of RBM, define execution plan and launch TF\n",
    "    def __init__(self, n_visible, n_hidden, learning_rate = 0.01, momentum = 0.8):\n",
    "        assert momentum >= 0.0 and momentum < 1\n",
    "        assert learning_rate >= 0.0 and learning_rate < 1\n",
    "        \n",
    "        '''\n",
    "        Initialize RBM variables\n",
    "        '''\n",
    "        self.n_visible     = n_visible\n",
    "        self.n_hidden      = n_hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum      = momentum\n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32, [None, self.n_visible])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.n_hidden])\n",
    "        \n",
    "        self.w     = tf.Variable(glorot_init(self.n_visible, self.n_hidden), dtype = tf.float32)\n",
    "        self.vbias = tf.Variable(tf.zeros([self.n_visible]), dtype = tf.float32)\n",
    "        self.hbias = tf.Variable(tf.zeros([self.n_hidden]), dtype = tf.float32)\n",
    "        \n",
    "        self.delta_w     = tf.Variable(tf.zeros([self.n_visible, self.n_hidden]), dtype = tf.float32)\n",
    "        self.delta_vbias = tf.Variable(tf.zeros([self.n_visible]), dtype = tf.float32)\n",
    "        self.delta_hbias = tf.Variable(tf.zeros([self.n_hidden]), dtype = tf.float32)\n",
    "        \n",
    "        '''\n",
    "        TF execution plan for Contrastive Divergence-k\n",
    "        '''\n",
    "        \n",
    "        # compute positive phase\n",
    "        ph_mean = tf.nn.sigmoid(tf.matmul(self.x, self.w) + self.hbias)\n",
    "        ph_sample = sample_bernoulli(ph_mean)\n",
    "        \n",
    "        # compute negative phase (k = 1)\n",
    "        nv = tf.matmul(ph_sample, tf.transpose(self.w)) + self.vbias\n",
    "        nh_mean = tf.nn.sigmoid(tf.matmul(nv, self.w) + self.hbias)\n",
    "        nh_sample = sample_bernoulli(nh_mean)\n",
    "        \n",
    "        # determine gradients on RBM parameters\n",
    "        gradient_w = tf.matmul(tf.transpose(self.x), ph_mean) - tf.matmul(tf.transpose(nv), nh_mean)\n",
    "        gradient_v = tf.reduce_mean(self.x - nv, 0)\n",
    "        gradient_h = tf.reduce_mean(ph_mean - nh_mean, 0)\n",
    "        \n",
    "        new_delta_w     = self.momentum * self.delta_w     + (1 - self.momentum) * self.learning_rate * gradient_w / tf.to_float(tf.shape(gradient_w)[0])\n",
    "        new_delta_vbias = self.momentum * self.delta_vbias + (1 - self.momentum) * self.learning_rate * gradient_v / tf.to_float(tf.shape(gradient_v)[0])\n",
    "        new_delta_hbias = self.momentum * self.delta_hbias + (1 - self.momentum) * self.learning_rate * gradient_h / tf.to_float(tf.shape(gradient_h)[0])\n",
    "        \n",
    "        # update weights and deltas\n",
    "        update_delta_w     = self.delta_w.assign(new_delta_w)\n",
    "        update_delta_vbias = self.delta_vbias.assign(new_delta_vbias)\n",
    "        update_delta_hbias = self.delta_hbias.assign(new_delta_hbias)\n",
    "        \n",
    "        update_w     = self.w.assign(self.w + new_delta_w)\n",
    "        update_vbias = self.vbias.assign(self.vbias + new_delta_vbias)\n",
    "        update_hbias = self.hbias.assign(self.hbias + new_delta_hbias)\n",
    "        \n",
    "        self.update_deltas  = [update_delta_w, update_delta_vbias, update_delta_hbias]\n",
    "        self.update_weights = [update_w, update_vbias, update_hbias]\n",
    "        \n",
    "        '''\n",
    "        TF execution plan for passing data forward and backward\n",
    "        '''\n",
    "        # compute values forward and backward\n",
    "        self.activation_mean   = tf.nn.sigmoid(tf.matmul(self.x, self.w) + self.hbias)\n",
    "        self.activation_sample = sample_bernoulli(self.activation_mean)\n",
    "        self.reconstruction    = tf.matmul(self.activation_sample, tf.transpose(self.w)) + self.vbias\n",
    "        self.decodification    = tf.matmul(self.y, tf.transpose(self.w)) + self.vbias\n",
    "        \n",
    "        # approximation to the reconstruction error\n",
    "        self.compute_err = tf.reduce_mean(tf.square(self.x - self.reconstruction))\n",
    "        \n",
    "        '''\n",
    "        Initialize TF Session\n",
    "        '''\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(init)\n",
    "    \n",
    "    # Forward and Backward functions\n",
    "    def forward(self, batch_x):\n",
    "        return self.sess.run(self.activation_sample, feed_dict = {self.x: batch_x})\n",
    "    \n",
    "    def backward(self, batch_y):\n",
    "        return self.sess.run(self.decodification, feed_dict = {self.y: batch_y})\n",
    "    \n",
    "    def forward_backward(self, batch_x):\n",
    "        return self.sess.run(self.reconstruction, feed_dict = {self.x: batch_x})\n",
    "    \n",
    "    def evaluate_error(self, batch_x):\n",
    "        return self.sess.run(self.compute_err, feed_dict = {self.x: batch_x})\n",
    "    \n",
    "    # Gibbs sampling for data generation\n",
    "    def gibbs_sampling(self, batch_x, n_gibbs = 30):\n",
    "        # Positive phase\n",
    "        ph_mean = tf.nn.sigmoid(tf.matmul(self.x, self.w) + self.hbias)\n",
    "        ph_sample = sample_bernoulli(ph_mean)\n",
    "        \n",
    "        # Negative phase\n",
    "        nh_sample = ph_sample\n",
    "        for i in range(n_gibbs):\n",
    "            nv = tf.matmul(nh_sample, tf.transpose(self.w)) + self.vbias\n",
    "            nh_mean = tf.nn.sigmoid(tf.matmul(nv, self.w) + self.hbias)\n",
    "            nh_sample = sample_bernoulli(nh_mean)\n",
    "        self.last_reconstructed = nv\n",
    "        \n",
    "        # TF compute\n",
    "        return self.sess.run(self.last_reconstructed, feed_dict = {self.x: batch_x})\n",
    "    \n",
    "    # How to train your RBM\n",
    "    def fit(self, data_x, n_epochs = 10, batch_size = 10, verbose = True):\n",
    "        assert n_epochs > 0\n",
    "        \n",
    "        n_data = data_x.shape[0]\n",
    "        \n",
    "        if batch_size > 0:\n",
    "            n_batches = n_data // batch_size + (0 if n_data % batch_size == 0 else 1)\n",
    "        else:\n",
    "            n_batches = 1\n",
    "        \n",
    "        data_x_cpy = data_x.copy()\n",
    "        inds = np.arange(n_data)\n",
    "        \n",
    "        errs = []\n",
    "        \n",
    "        for e in range(n_epochs):\n",
    "            \n",
    "            np.random.shuffle(inds)\n",
    "            data_x_cpy = data_x_cpy[inds]\n",
    "            \n",
    "            r_batches = range(n_batches)\n",
    "            epoch_errs = np.zeros((n_batches,))\n",
    "            epoch_errs_ptr = 0\n",
    "            \n",
    "            for b in r_batches:\n",
    "                batch_x = data_x_cpy[b * batch_size:(b + 1) * batch_size]\n",
    "                self.sess.run(self.update_weights + self.update_deltas, feed_dict = {self.x: batch_x})\n",
    "                epoch_errs[epoch_errs_ptr] = self.sess.run(self.compute_err, feed_dict = {self.x: batch_x})\n",
    "                epoch_errs_ptr += 1\n",
    "            \n",
    "            if verbose:\n",
    "                print('Epoch: {:d}'.format(e), 'Train error: {:.4f}'.format(epoch_errs.mean()))\n",
    "                sys.stdout.flush()\n",
    "            \n",
    "            errs = np.hstack([errs, epoch_errs])\n",
    "        \n",
    "        return errs\n",
    "    \n",
    "    # Additional functions\n",
    "    def get_weights(self):\n",
    "        return self.sess.run(self.w), self.sess.run(self.vbias), self.sess.run(self.hbias)\n",
    "    \n",
    "    def save_weights(self, filename, name):\n",
    "        saver = tf.train.Saver({name + '_w': self.w, name + '_v': self.vbias, name + '_h': self.hbias})\n",
    "        return saver.save(self.sess, filename)\n",
    "    \n",
    "    def set_weights(self, w, vbias, hbias):\n",
    "        self.sess.run(self.w.assign(w))\n",
    "        self.sess.run(self.vbias.assign(vbias))\n",
    "        self.sess.run(self.hbias.assign(hbias))\n",
    "    \n",
    "    def load_weights(self, filename, name):\n",
    "        saver = tf.train.Saver({name + '_w': self.w, name + '_v': self.vbias, name + '_h': self.hbias})\n",
    "        saver.restore(self.sess, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example: ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
